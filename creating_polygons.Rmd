---
title: "baseline_data_clean"
author: "Madeline Berger"
date: "11/8/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##Creating spatial data

This script takes GIS coordinates provided by the Punjab field team and converts them into polygons representing all the plots that were monitored as part of the experiment (both control and treatment groups)

```{r}
library(tidyverse)
library(sf)
library(stringr)
library(rgdal)
library(raster) #didn't end up using
library(maptools)
library(mapview) #to view subets 
library(here)


lat_lon <- read.csv(here("raw_spatial", "lag_lng_allbaseline.csv"))

head(lat_lon$latitude[1])

x = lat_lon$latitude[1]

#create a unique identifier for each group of vertices by merging the plot id column with the farmer id to create a new id

lat_lon_id <- lat_lon %>%
  mutate(unique_id=paste(substr(plot_id, 2, 2), resp_id))
  

lat_lon_id$unique_id <- gsub(" ", "", lat_lon_id$unique_id, fixed = TRUE) #this gets rid of the space, take space and sub it with nospace
  #unite("unique_id", plot_id, a_hhid, remove = FALSE)




```

##Part 1: Create and Export the Polygons

###Data Cleaning
```{r}
#find any values with Na

summary(lat_lon_id)

check_na <- lat_lon_id %>% 
  filter(is.na(a_hhid) | is.na(longitude)) #26 lines missing some data


#remove all NAs
#clean <- lat_lon[complete.cases(lat_lon), ]

clean_id <-lat_lon_id[complete.cases(lat_lon_id), ]
```



###Creating vectors: id_vec is the one used in the final version of loop
-unique id, resp Id

```{r}
#unique id 

id_vec <- unique(clean_id$unique_id) #make a list of all the values in unique_id column, use this one for loop

#try with resp id - more for data exploration

id_vec_resp <- unique(clean_id$resp_id)

#try with a_hhid -  more for data exploration

id_vec_ah <-unique(clean_id$a_hhid)


#old - from before it was clean, just to compare which are missing

id_vec_old <- unique(lat_lon_id$unique_id)

id_vec_resp_old <- unique(lat_lon$resp_id)

```



###Data Exploration
1. How many loops are we dealing with?
2. What are the first and last values?


```{r}

#1.How many unique resp_ids?
length(id_vec_resp)

head(id_vec_resp)

tail(id_vec_resp,1)

#2. How many unique_ids?

length(id_vec)

#4. How many a_hhid

length(id_vec_ah)

#4.Compared to old?

length(id_vec_old)

```



###Create subsets for testing loop (no need to run unless loop is amended)
```{r}
#with unique Id
#for testing
df_short <- lat_lon_id %>% 
   filter(unique_id == "110010033" | unique_id == "110010039" | unique_id == "110010075") 

#make a shorter list for the test set
id_vec_test <- unique(df_short$unique_id)


#with resp id 
df_resp <- lat_lon_id %>% 
    filter(resp_id == 101600981) %>% 
    as.matrix
```


###Filtered sets post-error - *you need to run this to make sure the loop is operating on the clean data set*
Removed: 1110600681, 1122201051, 1220301321, 1118200131
```{r}
#second
clean_id_2 <- clean_id %>% 
  filter(unique_id != 1110600681)

id_vec_2 <- unique(clean_id_2$unique_id)

#third

clean_id_3 <- clean_id_2 %>% 
  filter(unique_id != 1122201051)

id_vec_3 <- unique(clean_id_3$unique_id)

#fourth

clean_id_35 <- clean_id_3 %>% 
  filter(unique_id != 1220301371)

clean_id_4 <- clean_id_35 %>% 
  filter(unique_id != 1118200131)

id_vec_4 <- unique(clean_id_4$unique_id)

```



Code to add another point for closure - if needed, not necessary to run the loop
```{r}
#making matrices and testing for closure

 id = id_vec[i]
  
df <- lat_lon_id %>% 
    filter(unique_id == id) %>% 
    select(longitude, latitude) %>% 
    as.matrix

line_sf <- st_linestring(df)

plot(line_sf)

#check whether the first and last rows are the same point: 

df[1, ] == df[nrow(df), ]

#bind the matrix with its own first row to close it

df_closed <- rbind(df, df[1, ])

#check again if the first and last rows are closed

df_closed[1, ] == df_closed[nrow(df_closed), ]

line_sf_closed <- st_linestring(df_closed)

plot(line_sf_closed)


```

**Loop to make polygons - you must use 'clean_id_4' for it to run properly**
```{r}
for(i in seq_along(unique(clean_id_4$unique_id))){
  #filter the df 
  print(i)
  
  id = id_vec_4[i]
  
  df <- clean_id_4 %>% 
    filter(unique_id == id) %>% 
    dplyr::select(longitude,latitude)
    as.matrix
    
  print(i)
  #get the coodrinates
  #create the polygon, and add attributes
  poly <- st_sf(data.frame(unique_id = id, 
                           st_sfc(st_polygon(list(as.matrix(df))),crs = 4326)))
  
 print(i)
  
 if(i == 1) {
   poly_sf <- poly
 } else {
   poly_sf <- rbind(poly_sf, poly)
 }
 
print(i) #used these when building the loop to find where it was breaking, and then filtered those out above. 
  
}

```


Bind poly_id, farmer id, and family id to poly_sf 

```{r}

#create table of IDs minus the lat lon data

info <- clean_id_4[-c(1,4:5)]


#merge  - tried three ways

poly_complete_merge <- merge(info, poly_sf, by = "unique_id") # run this first

clean_done <- poly_complete_merge[!duplicated(poly_complete_merge$unique_id), ] #filter out duplicates that result from the merge

```

Select random sample of 200 for verification 

```{r}

clean_sample <- sample_n(clean_done, 200)
#just randomly selecs 200 rows from the specified data frame, dplyr

info_sample <- clean_sample[-c(4)]

#create a data set with the random sample removed, per Kelsey's request

#use `anti_join` from the tidverse to take out the 200 sample

#clean_done_df <- as.data.frame(clean_done)

#clean_sample_df <- as.data.frame(clean_sample)

all_minus_sample <- anti_join(clean_done, clean_sample, by = "unique_id")

#convert back to spatial object
#all_minus_sample_sf <- st_as_sf(x = all_minus_sample, sf_column_name = "geometry") #this didn't work 

```


Writing out shapefiles and csv  (don't run if they already exist)
```{r}
st_write(poly_sf, "test_noinfo.shp")

st_write(clean_done, "test_all.shp") #this has all the info, is the finalized version that was imported into arc

st_write(clean_sample, "random_sample.shp") #200 for verification

st_write(all_minus_sample_sf, "all_minus_sample.shp") #all minus the sample

#these were all moved to the outputs folder once they were exported


##write out csvs: 
write_csv(clean_id, "H:/punjab_project/punjab_project/ids_2.csv", col_names = TRUE)

write_csv(clean_id_3, "H:/punjab_project/punjab_project/ids_3.csv", col_names = TRUE)

write_csv(check_na, "H:/punjab_project/punjab_project/removed.csv", col_names = TRUE)

write_csv(clean_done,"H:/punjab_project/punjab_project/clean_poly_info.csv", col_names = TRUE )
```


Write out kml file for Google Earth or MyMaps
```{r}
#first need to create a spatial polygon from the dataframe - this isn't working

SpatialPolygonsDataFrame(poly_sf, info)

writeOGR(clean_sample, dsn="random_sample.kml", layer = "sample_plots", driver = "KML")

```

The below script can now be found in the `updating_2019_data.Rmd` 
<!-- ##Part 2: Add survey data, updated -->

<!-- 1. Read in new data -->
<!-- ```{r} -->
<!-- #survey data -->
<!-- monitoring <- read_csv(here("raw_csv","monitoring_updated.csv")) %>%  -->
<!--   dplyr::select(1:5, starts_with("pv")) %>%  -->
<!--   mutate( -->
<!--     unique_id = paste(substr(plot_id, 2,2), resp_id, sep = "") -->
<!--   ) -->

<!-- spotcheck <- read_csv(here("survey_data","spot_check_updated.csv")) %>% -->
<!--   dplyr::select(1:3, starts_with("pv")) %>%  -->
<!--    mutate( -->
<!--     unique_id = paste(substr(plot_id, 2,2), resp_id, sep = "") -->
<!--   ) -->

<!-- #spatial polygons -->
<!-- polygons <- read_sf(here(dsn = "outputs_mb"), layer = "test_all") -->


<!-- ``` -->

<!-- What is burned? -->
<!-- Monitoring data:  -->
<!-- - pv_burn_any == 1 means it was burned, either main or alt -->
<!-- - bl_error = plot is in monitoring but not baseline -->

<!-- Spot check -->
<!-- - pv_burn = if any type of burning is noticed (1 of 7) -->


<!-- 7 types of burning measured:  -->
<!-- 1.	 sc_burn_straw_stub "Straw or stubble looks burnt/partially burnt." -->
<!-- 2.	sc_burn_straw_ash  "Black/grey ash on the soil surface." -->
<!-- 3.	sc_burn_resid  "Root residues or stubble/stem residues appear burnt." -->
<!-- 4.	sc_burn_stand_stub Standing stubble appears burnt from the top." -->
<!-- 5.	sc_burn_grass  "Burnt grass and weeds on the plot boundaries." -->
<!-- 6.	sc_burn_trees_burnt  "Burnt leaves/branches of the trees on the plot boundary." -->
<!-- 7.	sc_burn_realtime "Stubble's burning in real time." -->

<!-- 2. Create df with burn column indicating "1" if any were burned -->
<!-- ```{r} -->
<!-- #join monitoring data using unique id -->
<!--  #this stands for updated monitoring and spotcheck data msc -->

<!-- updated_2019_mon <- inner_join(polygons, monitoring, by = "unique_id") %>%  -->
<!--   dplyr::select(-resp_id.x) -->

<!-- burn_alt_only_mon <- updated_2019_mon %>%  -->
<!--   dplyr::select(unique_id, resp_id.y, pv_burnt_alt, geometry) -->

<!-- #there are 4 fewer lines - this makes sense because there were two in the `bl_error`, and since each entry had two lines for two different dates, a total of 4 should be missing.  -->

<!-- ``` -->

<!-- 3. Create df with burn column indicating "1" only for main burning -->
<!-- ```{r} -->
<!-- #join spot check data  -->
<!-- updated_2019_sc <- inner_join(polygons, spotcheck, by = "unique_id" ) %>%  -->
<!--   dplyr::select(-resp_id.x) -->

<!-- burn_alt_only_sc <- updated_2019_sc %>%  -->
<!--   dplyr::select(unique_id, resp_id.y, pv_burnt_alt, geometry) -->
<!-- #there are two less - also makes sense because there were two erros that were not in baseline and this one did not have doubles -->

<!-- ``` -->


<!-- 4. Create df with burn column indicating "1" only for alternate burning -->

<!-- ```{r} -->
<!-- #row bind both -->
<!-- updated_2019_mon_sc <- rbind(burn_alt_only_mon, burn_alt_only_sc) -->

<!-- #seems like there are duplicates - remove (may have lost data?) -->

<!-- updated_2019_mon_sc <- updated_2019_mon_sc[!duplicated(updated_2019_mon_sc$unique_id), ] -->

<!-- ``` -->

<!-- 5. Export new dataset -->
<!-- ```{r} -->

<!-- st_write(updated_2019_mon_sc, "updated_2019_mon_sc.shp") -->


<!-- ``` -->


<!-- Next step raster data:  -->

<!-- ##Read in raster data -->
<!-- Start with week1_90 from BAMSA -->
<!-- ```{r} -->

<!-- week1_90 <- raster("week1_90.tif") -->

<!-- #look at it -->


<!-- mapview(week1_90) -->


<!-- #cool! What are the colors for? -->


<!-- ex <- mapview(clean_poly_ablu, layer.name = "Plots")+ -->
<!--   mapview(week1_90, layer.name = "Burn scars") -->

<!-- mapshot(ex, url = paste0(getwd(), "/example.html")) -->

<!-- ``` -->


<!-- ##Reclassify raster to binary  -->
<!-- ```{r} -->


<!-- #we want everything besides 0 to be 1 and 0 to stay zero -->
<!-- #first create a vector that will become the matrix you feed to the reclass function -->

<!-- m <- c(0,0,0,1,255,1) -->

<!-- #create matrix -->
<!-- rclmat <- matrix(m, ncol=3, byrow=TRUE) -->

<!-- rclmat -->

<!-- week1_90_rcl <- reclassify(week1_90, rclmat, include.lowest=TRUE) -->

<!-- #look at it -->

<!-- mapview(week1_90_rcl) -->
<!-- ``` -->


















##################################################

Tried but unsuccessful: loop that tests resp_id

```{r}
#testing grouping
group_by_resp <- lat_lon %>% 


#new loop 

for(i in seq_along(unique(lat_lon$resp_id))){
  #filter the df 
  id = id_vec_resp[i]
  
  df <- lat_lon %>% 
    filter(resp_id == id) %>% 
    select(resp_id,latitude, longitude)
   
    
  if(df[1,2] != df[nrow(df),2]){  
    print()
  }
}

#poly <- st_sf(data.frame(resp_id = id, st_sfc(st_polygon(list(as.matrix(df))),crs = 4326)))

###############################
```


Build polygons to test certain IDs
```{r}

#using resp id

df_test_resp <- lat_lon %>% 
  filter(resp_id == 100100331) %>% 
  select(latitude,longitude)

df_test_resp
  
poly <- st_sf(data.frame(resp_id = 100100331, st_sfc(st_polygon(list(as.matrix(df_test_resp))),crs = 4326)))


#using unique id

df_test_uniq <- clean_id_3%>% 
  filter(unique_id == 1122201051) %>% 
  select(latitude,longitude)


poly_uniq <- st_sf(data.frame(resp_id = 100100331, st_sfc(st_polygon(list(as.matrix(df_test_uniq))),crs = 4326)))

df_test_uniq

#so this works but the one above does not
```

Overall - NONE of these IDs are unique to the polygon!!


######################################

Notes from kelsey
a_hhid = farmer / household id 
missing for 25 observations
  
resp_id = 
is never missing 

some a_hhid have more than one 

one missing logtude 

split this id so that you can have a farmer plot panel

when you get a dataset at the plot level 

Priority be: shapefile available in whatever form before you go offline 

Then start working more with the shapefile to merge in information like "was that plot burned"

